% chktex-file 8
\hypertarget{sec:1}{\section{Diskrete Verteilungen}}
\subsection*{Diskrete Gleichverteilung}
Die diskrete Gleichverteilung auf einer endlichen Menge $\W = \{ x_1, \dots,
  x_n \}$ gehört zu einer Zufallsvariablen $X$ mit Wertebereich $\W$ und
Gewichtsfunktion:
\begin{align*}
  p_X (x_k) = P[X = x_k] = \frac{1}{N} &  & k \in \{1, \dots, N\}
\end{align*}
\subsection*{Unabhängige 0-1-Experimente}
Es sei $A_i := \{\text{Erfolg beim $i$-ten Experiment}\}$ und:
\begin{itemize}
  \item Die $A_i$ sind unabhängig
  \item $P[A_i] = p$ für alle $i$
  \item \[
          Y_i (\omega) =
          \begin{cases}
            1 & \omega \in A_i      \\
            0 & \omega \not \in A_i
          \end{cases}
        \]
\end{itemize}
\subsection*{Bernoulli-Verteilung}
Ein einziges 0-1-Experiment mit $\W (X) = \{0, 1\}$. Die Gewichtsfunktion ist
gegeben durch $p_X (1) = p$, sowie $p_X (0) = 1-p$. Man schreibt kurz $X \sim
  Be (p)$. Es gilt:
\begin{align*}
  E[X]   & = 1 \cdot P[X = 1] + 0 \cdot P[X = 0] = p \\
  Var[X] & = E[X^2] - {E[X]}^2 = p \cdot  (1-p)
\end{align*}
\subsection*{Binomialverteilung}
Beschreibt die Anzahl der Erfolge bei $n$ unabhängigen 0-1-Experimenten mit
Erfolgsparameter $p$. Also ist die Zufallsvariable respektive Gewichtsfunktion:
\begin{align*}
  X       & = \Sn I_{A_i} = \Sn Y_i                                \\
  p_X (k) & = P[X = k] = \binom{n}{k} \cdot p^k \cdot  (1-p)^{n-k}
\end{align*}
und man schreibt kurz $X \sim Bin (n, p)$. Es gilt weiter:
\begin{align*}
  E[X]   & = \Sn E[Y_i] = n \cdot p                \\
  Var[X] & = \Sn Var[Y_i] = n \cdot p \cdot  (1-p)
\end{align*}
\subsection*{Geometrische Verteilung}
Bei einer unendlichen Folge von unabhängigen 0-1-Experimenten mit
Erfolgsparameter $p$ sein $X$ die Wartezeit zum ersten Erfolg:
\begin{align*}
  X       & = \inf \{ i \in \N : A_i \text{ tritt ein} \} \\
  p_X (k) & = P[X = k] = p \cdot  (1-p)^{k-1}
\end{align*}
wir schreiben $X \sim Geom (p)$ und es gilt:
\begin{align*}
  E[X]              & = \sum_{i = 0}^\infty  (1-p)^l
  = \frac{1}{1- (1-p)} = \frac{1}{p}                 \\
  E[X \cdot  (X-1)] & = \frac{2 (1-p)}{p^2}          \\
  Var[X]            & = \frac{1-p}{p^2}
\end{align*}
\subsection*{Negativbinomiale Verteilung}
Bei einer unendlichen Folge von unabhängigen 0-1-Experimenten mit
Erfolgsparameter $p$ sein $X$ die Wartezeit zum $r$-ten Erfolg ($r \in \N$):
\begin{align*}
  X       & = \inf \{ k \in \N : \sum_{i = 1}^k I_{A_i} = r \}         \\
  p_X (k) & = P[X = k] = \binom{k-1}{r-1} \cdot p^r \cdot  (1-p)^{k-r}
\end{align*}
wir schreiben $X \sim NB (r, p)$ und es gilt:
\begin{align*}
  E[X]   & = \sum_{i = 1}^r E[X_i] = \frac{r}{p}                  \\
  Var[X] & = \sum_{i = 1}^r Var[X_i] = \frac{r \cdot  (1-p)}{p^2}
\end{align*}

\subsection*{Hypergeometrische Verteilung}
In einer Urne seien $n$ Gegenstände, davon $r$ vom Typ $1$ und $n-r$ vom Typ
$2$. Man zieht ohne zurücklegen $m$ der Gegenstände. Die Zufallsvariable $X$
beschreibt die Anzahl der Gegenstände vom Typ $1$ in der Stichprobe. Der
Wertebereich von $X$ ist $\W (X) = \{0, 1, \dots, \min (m, r)\}$ und:
\begin{align*}
  p_X (k) & = \frac{\binom{r}{k} \cdot \binom{n-r}{m-k}}{\binom{n}{m}}
          &                                                                                 & \text{für } k \in \W (X)                             \\
  E[X]    & = \Sn i \cdot p_X (i) = m \cdot \frac{r}{n}                                     &                          & \text{ (Nicht im Skript)} \\
  Var[X]  & = m \cdot \frac{r}{n} \left  (1 - \frac{r}{n} \right) \cdot \frac{n - m}{n - 1} &                          & \text{ (Nicht im Skript)}
\end{align*}
\subsection*{Poisson Verteilung}
Die Poisson Verteilung mit Parameter $\lambda \in (0, \infty)$ ist eine
Verteilung auf der Menge $\N_0 = \{0, 1, 2, \dots\}$ mit Gewichtsfunktion:
\begin{align*}
  p_X (k) & = e^{-\lambda} \cdot \frac{\lambda^k}{k!}
          &                                                                                            & \text{für } k = 0, 1, 2, \dots \\
  E[X]    & = \Sn i \cdot \frac{\lambda^i}{i!} e^{-\lambda} = \lambda e^{-\lambda} e^\lambda = \lambda                                  \\
  E[X^2]  & = \lambda^2 + \lambda                                                                                                       \\
  Var[X]  & = \lambda
\end{align*}
Ist eine Zufallsvariable $X$ Poisson verteilt mit Parameter $\lambda$
schreiben wir $X \sim P (\lambda)$.