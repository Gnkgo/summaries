\section{Unitary Transforms}
\greenbf{Vectorization:} interpret image as vector row-by-ow: \graytext{$I = \begin{bmatrix}
    \begin{smallmatrix}
        1 & 2 & 3\\
        4 & 5 & 6
    \end{smallmatrix}
\end{bmatrix} \rightarrow \begin{bmatrix}
    \begin{smallmatrix}
        1 & 2 & 3 & 4 & 5 & 6
    \end{smallmatrix}
\end{bmatrix}$}\\
\greenbf{linear image processing:} can be written as $\vec{g} = H\vec{f}$\\
\greenbf{Image collection (IC):} $F = [f_1, f_2... f_n]$\\
\greenbf{Autocorrelation matrix} $Rff = \frac{F \cdot F^H}{N}$ its Eigenvector with largest Eigenvalue is direction of largest variance among pictures.\\
\greenbf{Unitary transform:} for transform $A$ iff $A^H = A^{-1}$ \graytext{if real-valued $\rightarrow$ orthonormal}\graytext{every unitary transform is a rotation + sign flip, length conserved}\\
\greenbf{Energy conservation:} $||\vec{C}||^{2} = \vec{C}^{H}C = \vec{f}^{H}A^{H}Af = ||\vec{f}||^{2}$ \\
\subsection*{Karhunen-Loeve Transform \graytext{Same as PCA. Order by decreasing eigenvalues}}
\greenbf{Energy concentration property:} no other unitary transform packs as much energy in the first $J$ coefficients \graytext{(for arbitrary $J$)} and mean squared approximation error by choosing only first $J$ coefficients is minimized.\\
\greenbf{Optimal energy concentratioin of KLT} consider truncated coefficient vector $\vec{b} = I_J \vec{c}$ \graytext{($I_J$: identity matrix with first J columns)} Energy in first $J$ coefficients for an arbitrary transform $A : E = Tr(R_{bb}) = Tr(I_J R_{cc} I_{J}) = Tr(I_J A R_{ff} A^H I_J) = \sum_{k = 0}{J = 1} a_k^T R_{ff} a_k^*$ where $a_k^T$ is $k-th$ row of $A$. Lagrangian cost function to enforce unit-length basis vectors: 
$L = E + \sum_{k = 0}^{J - 1} \lambda_k (1 - a_k^T a_k^*) = \sum_{k = 0}^{J - 1} a_k^T R_{ff} a_k^* + \sum_{k = 0}^{J - 1} \lambda_k (1 - a_k^T a_k^*)$\\ 
Differentiating $L$ with respect to $a_j$: $R_{ff} a_j^* = \lambda_i a_j^* \quad \forall_j < J$ \graytext{necessary condition}
\subsection*{Simple recognition}
SSD between images, best match wins \graytext{very expensive, since need to correlate with every image}
\subsection*{Principle Component analysis \graytext{PCA}}
\greenbf{Steps:} standardize data, get Eigenvectors and values from covariance matrix or do SVD, sort Eigenvalues and vectors in descending order get $j$ largest components, construct projection matrix from selected $j$ Eigenvectors transform dataset by multiplying with projection matrix.\\
TODO\\
\greenbf{Uses of PCA:} lossycompression by keeping only the most important $k$ components. Face recognition \graytext{eigenfaces} and face detection.
\subsection*{Eigenspace matching}
Do PCA \graytext{with mean subtraction} and get closest rank-$k$ approximation of database images \graytext{(eignfaces)} \\
For a new query: normalize, subtract mean \graytext{(of database)} project to subspace then do similarity matching with eigenfaces.
\subsection{Fischerfaces:} 
Find directions where ratio between / within individual variance is maximized. Linearly project to basis where dimension with good signal: noise ratio is maximized. \\
$W_{\text{opt}} = \arg\max_W \frac{\det(W R_B W^H)}{\det(W R_W W^H)}, R_b = \sum_{i = 1}^{c} (\mu_i - \mu)(\mu_i - \mu)^T, R_W = \sum_{i = 1}^{c} \sum_{x \in X_i} (x - \mu_i)(x - \mu_i)^T$\\
