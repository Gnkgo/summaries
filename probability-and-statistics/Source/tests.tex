\section{Statistik}
\subsection{$\X^2$-Verteilung}
Die $\X^2$- Verteilung mit $n$ Freiheitsgraden (bez. $\X^2_n$) gehört zu einer
stetigen Zufallsvariable $Y$ mit Dichtefunktion:
\begin{align*}
  f_Y (y) = \frac{1}{2^{\frac{n}{2}}  \Gamma (\frac{n}{2})} y^{\frac{n}{2} - 1} e^{-\frac{1}{2} y}
\end{align*}
wobei dies ein Spezialfall der $Ga (\alpha, \lambda)$ Verteilung ist mit
$\alpha = \frac{n}{2}$ und $\lambda = \frac{1}{2}$. Sind die Zufallsvariablen
$\zufallsvariablen$ i.i.d. $\sim \Standardnormalverteilt$, so ist die Summe
$Y := \Sn X_i^2 \sim \X^2_n$.
\subsection{$t$-Verteilung}
Die $t$-Verteilung mit $n$ Freiheitsgraden gehört zu einer stetigen
Zufallsvariable $Z$ mit Dichtefunktion
\begin{align*}
  f_Z (z) = \frac{\Gamma (\frac{n+1}{2})}{\sqrt{n \pi} \, \Gamma (\frac{n}{2})} \left( 1 + \frac{z^2}{n} \right)^{-\frac{n+1}{2}}
   &  & \text{für } z \in \R
\end{align*}
für $n = 1$ ist das eine Cauchy Verteilung und für $n \ra \infty$ erhält man
$\Standardnormalverteilt$. Sind $X, Y$ unabhängig und $X \sim \Standardnormalverteilt$
und $Y \sim \X^2_n$, so ist der Quotient:
\begin{align*}
  Z := \frac{X}{\sqrt{\frac{1}{n} Y}} \sim t_n \text{, also $t$-Verteilt mit $n$ Freiheitsgraden}
\end{align*}
\section{Tests}
\subsection{Hypothesen}
Es gibt:
\begin{itemize}
  \item Hypothese $H_0 : \vt \in \varTheta_0$
  \item Alternative $H_A : \vt \in \varTheta_A$
\end{itemize}
Man verwirft die Hypothese genau dann, wenn der realisierte Wert im
Verwerfungsbereich $K$ liegt.
\subsection{Fehler}
Es gibt folgende Fehler:
\begin{itemize}
  \item 1. Art: Hypothese wird zu unrecht abgelehnt. $P_\vt[T \in K]$ für $\vt \in \varTheta_0$
  \item 2. Art: Hypothese wird zu unrecht nicht verworfen. $P_\vt[T \not \in K]$ für $\vt \in \varTheta_A$
\end{itemize}


Meisst kann man nicht beides minimieren, also geht man wie folgt vor:
\begin{enumerate}
  \item Man wählt ein \textbf{Signifikanzniveau} $\alpha \in (0, 1)$ und kontrolliert
        die Wahrscheinlichkeit eines Fehlers 1. Art durch
        \begin{align*}
          \sup_{\vt \in \varTheta_0} P_\vt[T \in K] \leq \alpha
        \end{align*}
  \item Man versucht die Wahrscheinlichkeit für einen Fehler zweiter Art $P_\vt[T \not
            \in K]$ für $\vt \in \varTheta_A$ zu minimieren. Dazu maximiert man die
        \textbf{Macht des Tests}:
        \begin{align*}
          \beta : \varTheta_A \mapsto [0, 1] &  & \vt \mapsto \beta (\vt) := P_\vt[T \in K]
        \end{align*}
\end{enumerate}
Somit ist es schwieriger eine Hypothese zu verwerfen als zu behalten. In einem
Test \textbf{verwendet man deshalb immer als Hypothese die Negation der eigentlich gewünschten Aussage.}
\BoxStart{}
\subsection{Beispiel:}
\textbf{Was passiert, wenn das Signifikanzniveau $\alpha$ kleiner wird?}
Kleineres Signifikanzniveau ($\alpha$)
\begin{itemize}
  \item Engerer Verwerfungsbereich
  \item Weniger extreme Teststatistikwerte für Ablehnung der Nullhypothese erforderlich
  \item Strengerer Test
  \item Höhere Anforderungen an statistische Signifikanz
  \item Geringere Wahrscheinlichkeit, Nullhypothese fälschlicherweise abzulehnen
  \item Höhere Wahrscheinlichkeit für Fehler vom Typ II (falsche Akzeptanz der Nullhypothese, wenn sie in Wirklichkeit falsch ist)
  \item Macht wird reduziert
\end{itemize}
\BoxEnd{}

\BoxStart{}
\subsection{Beispiel: Hypothesen-Test}
Wir haben eine Münze mit einer Seite rot und der anderen Seite
blau gefärbt, und wir vermuten, dass die Münze gezinkt ist und eher auf der
blauen Seite landet.
\begin{enumerate}[leftmargin=0.4cm]
  \item \textbf{Modell:} Unter $P_p$ sind die $X_i$ i.i.d., $\sim Ber(p), i = 1, \ldots, 10, p$ unbekannt
  \item \textbf{Nullhypothese: }$H_0 : p = p_0 = 0.5$
  \item \textbf{Alternativhypothese: } \(H_A: p = p_A > p_0\).
  \item \textbf{Teststatistik: } \(T = \sum_{i=1}^{10} X_i\), denn
  \begin{align*}
    &R(x_1, \ldots , x_{10}; \lambda_0, \lambda_A) \\
    &= L(x_1, \ldots , x_{10}; \lambda_0) L(x_1, \ldots , x_{10}; \lambda_A)\\ 
    &= \frac{p_0^T (1 - p_0)^{10-T}}{p_A^T (1 - p_A)^{10-T}}\\
    &= \left(\frac{p_0(1 - p_A)}{p_A(1 - p_0)}\right)^T \left(\frac{1 - p_0}{1 - p_A}\right)^{10}
  \end{align*}
  Da \(\frac{p_0(1-p_A)}{p_A(1-p_0)} < 1\) wird \(R(x_1, \ldots , x_{10}; p_0, p_A)\) klein, genau dann, wenn \(\sum_{i=1}^{10} x_i\) groß ist. Wir wählen als Teststatistik also \(T = \sum_{i=1}^{10} X_i\).
  \item \textbf{Verteilung der Teststatistik unter \(H_0\): } \(T \sim \text{Bin}(10, 1/2)\).
  \item \textbf{Verwerfungsbereich: } Der kritische Bereich "Quotient klein" hat die äquivalente Form "Summe groß", also ist der Verwerfungsbereich von der Form \(K = (k, \infty)\). Um das Signifikanzniveau einzuhalten, muss gelten
  \[ P_{p_0}[T \in K] = P_{p_0}[T > k] \leq 1\% \Rightarrow P_{p_0}[T \leq k] \geq 99\%. \]
  Deshalb haben wir als Verwerfungsbereich \(K = (9, \infty)\).
  \item \textbf{Beobachteter Wert der Testst.: } \(t = T(\omega) = 8\).
  \item \textbf{Testentscheid: } Da 8 nicht im Verwerfungsbereich liegt, wird die Nullhypothese nicht verworfen.
    

\end{enumerate}
\BoxEnd{}
\subsection{Hypothesis testing}
\begin{enumerate}
  \item \textbf{Left-sided or left-tailed test: } If $H_0 : p = p_0$ is tested against the alternative $H_1 : p < p_0$, the region of rejection has the form $\{0, 1, \ldots, k\}$. The critical value $k$ is the largest value such that $P(X \leq k) \leq \alpha$.
  \item \textbf{Right-sided or right-tailed test: } If \(H_0 : p = p_0\) is tested against the alternative \(H_1 : p > p_0\), the region of rejection has the form \(\{k, k + 1, \ldots , n\}\). The critical value \(k\) is the smallest value such that \(P(X \geq k) \leq \alpha\).
  \item \textbf{Two-sided or two-tailed test: } If \(H_0 : p = p_0\) is tested against the alternative \(H_1 : p \neq p_0\), the region of rejection has the form \(K = \{0, 1, \ldots , k_1\} \cup \{k_2, k_2+1, \ldots , n\}\). The critical values \(k_1\) and \(k_2\) are determined as the smallest and largest value, respectively, such that \(P(X \leq k_1) \leq \frac{\alpha}{2}\) and \(P(X \geq k_2) \leq \frac{\alpha}{2}\).

\end{enumerate}

\subsection{Likelihood Quotient}
Sei $L (x_1, \dots, x_n; \vt)$ die Likelihood Funktion und $\vt_0 \in
  \varTheta_0$ sowie $\vt_A \in \varTheta_A$. Dann definieren wir:
\begin{align*}
  R (x_1, \dots, x_n; \vt_0, \vt_A) = \frac{L (x_1, \dots, x_n; \vt_A)}{L (x_1, \dots, x_n; \vt_0)}
\end{align*}
je grösser der Quotient, desto wahrscheinlicher die Alternative. Es gibt auch:
\begin{align*}
  R (x_1, \dots, x_n)             & = \frac{ \sup_{\vt \in \varTheta_A} L (x_1, \dots, x_n; \vt)}{\sup_{\vt \in \varTheta_0} L (x_1, \dots, x_n; \vt)}                  \\
  \widetilde{R} (x_1, \dots, x_n) & = \frac{ \sup_{\vt \in \varTheta_A \cup \varTheta_0} L (x_1, \dots, x_n; \vt)}{\sup_{\vt \in \varTheta_0} L (x_1, \dots, x_n; \vt)} \\
\end{align*}
Wähle Konstante $c_0$ für $K_0 =  (c_0, \infty)$ mithilfe von Signifikanzniveau.
\subsection{Neyman-Pearson Lemma}
Sei $\varTheta_0 = \{\vt_0\}$ und $\varTheta_A = \{\vt_A\}$. Wie oben sei $T =
  R (\zufallsvariablen; \vt_0, \vt_A)$ und $K := (c, \infty)$, sowie $\alpha^* :=
  P_{\vt_0}[T \in K] = P_{\vt_0}[T > c]$. Der Likelihood Quotienten Test mit
Teststatistik $T$ und kritischem Bereich $K$ ist dann in folgendem Sinn
optimal: Jeder andere Test mit Signifikanzniveau $\alpha \leq \alpha^*$ hat
eine kleinere Macht (bez. Grössere WS Fehler 2. Art).
\subsection{$p$-Wert}
(Nach Wikipedia) Der $p$-Wert ist die Wahrscheinlichkeit ein mindestens
so extremes Testergebnis zu erhalten, wenn die Nullhypothese gelten würde:
\begin{align*}
  p (x) & = \cond{X \leq x}{H_0} \text{   oder   } \cond{X \geq x}{H_0}   \\
  p (x) & = 2 \cdot \min \{ \cond{X \leq x}{H_0}, \cond{X \geq x}{H_0} \}
\end{align*}
\begin{definition}{Binomialtest}
  \color{red}Test für den Erfolgswahrscheinlichkeit bei bekannter Anzahl Versuche. \color{black} Hier sind
  $\zufallsvariablen$ i.i.d. $\sim \text{Ber}(p)$ und wir möchten die
  Hypothese $H_0 : p = p_0$ testen. Mögliche Alternativen $H_A$ sind $p > p_0$, $p < p_0$ (einseitig) oder $p \neq p_0$ (zweiseitig). Die Teststatistik ist:
  \begin{align*}
    T & = \sum_{i=1}^{n} X_i \sim \text{Bin}(n, p_0) &  & \text{unter } P_{p_0}
  \end{align*}
\[
R(x_1, \ldots, x_{10}; \lambda_0, \lambda_A) = \frac{L(x_1, \ldots, x_{10}; \lambda_0)}{L(x_1, \ldots, x_{10}; \lambda_A)}
\]
\[
= \frac{p_0^T (1 - p_0)^{10-T}}{p_A^T (1 - p_A)^{10-T}}
\]
\[
= \left( \frac{p_0(1 - p_A)}{p_A(1 - p_0)} \right)^T \left( \frac{1 - p_0}{1 - p_A} \right)^{10}
\]

Da $\left( \frac{p_0(1-p_A)}{p_A(1-p_0)} \right) < 1$ wird $R(x_1, \ldots, x_{10}; p_0, p_A)$ klein, genau dann, wenn $\sum_{i=1}^{10} x_i$ groß ist. 

  Und die Verwerfungsbereich:
  \begin{align*}
    p < p_0    &  & (0, z_\alpha)                                                      \\
    p > p_0    &  & (z_{1 - \alpha}, n)                                                 \\
    p \neq p_0 &  & (0, z_{\frac{\alpha}{2}}) \cup  (z_{1 - \frac{\alpha}{2}}, n)
  \end{align*}


  
\end{definition}
\begin{definition}{$z$-Test}
\color{red}Normalverteilung, Test für Erwartungswert bei bekannter Varianz. \color{black} Hier sind
$\zufallsvariablen$ i.i.d. $\mathcal{N} (\vt, \sigma^2)$. Wir möchten die
Hypothese $H_0 : \vt = \vt_0$ testen. Mögliche Alternativen $H_A$ sind $\vt >
  \vt_0$, $\vt < \vt_0$ (einseitig) oder $\vt \neq \vt_0$ (zweiseitig). Die
Teststatistik ist:
\begin{align*}
  T & = \frac{\overline{X} - \vt_0}{\sigma / \sqrt{n}} \sim \Standardnormalverteilt
    &                                                                               & \text{unter } P_{\vt_0}
\end{align*}
Und die Verwerfungsbereich:
\begin{align*}
  \vt < \vt_0    &  & (-\infty, z_\alpha)                                                      \\
  \vt > \vt_0    &  & (z_{1 - \alpha}, \infty)                                                 \\
  \vt \neq \vt_0 &  & (-\infty, z_{\frac{\alpha}{2}}) \cup  (z_{1 - \frac{\alpha}{2}}, \infty)
\end{align*}
Wobei die $z$ Werte in der Tabelle nachgeschaut werden können
und es gilt $z_\alpha = - z_{1 - \alpha}$.
\end{definition}
\BoxStart{}
\subsection{Beispiel Fehler 1. Art berechnen}
Sei $\Theta = [0, 1]$ und seien $X_1, \ldots , X_6$ unabhängig, identisch verteilt unter $P_\theta$
mit $X_i \sim \text{Ber}(\theta)$. Wir betrachten die Nullhypothese $H_0 : \theta = \frac{1}{2}$ und die Alternativhypothese
$H_A : \theta = \frac{1}{3}$. Wie hoch ist die Wahrscheinlichkeit eines Fehlers 1. Art für den Test $(T, K)$ mit
$T = \sum_{i=1}^{6} X_i$ und $K = (-\infty, 0]$?

Die Teststatistik $T$ ist die Summe der beobachteten Werte der $X_i$:
\begin{align*}
  P(T \in K)  &= P[T \leq 0] = P[T = 0] \\
              &= P(X_1 = 0) \cdot P(X_2 = 0) \cdot \ldots \cdot P(X_6 = 0)\\
              &= \left( \frac{1}{2} \right)^6 = \frac{1}{64} 
\end{align*}



\BoxEnd{}
\BoxStart{}
\subsection{Beispiel Fehler 2. Art berechnen}
Nehme an: einseitiger $z$-Test, $T = \frac{\overline{X}_n - \mu_0}{\sigma /
    \sqrt{n}}$, $\mu_0 = 70$.
\begin{align*}
  H_0 : \mu = \mu_0 &  & H_A : \mu < \mu_0
\end{align*}
Kritischer Bereich mit $5$\% niveau: $K =  (- \infty, -1.645)$. Wir
nehmen an, dass $T = \frac{\overline{X}_n - \mu_A}{\sigma / \sqrt{n}} \sim \Standardnormalverteilt$:
\begin{align*}
  P_{\mu_A}[T \not \in K]
   & = P_{\mu_A}[T > -1.645]                                                                                                            \\
   & = P_{\mu_A} \left[\frac{\overline{X}_n - \mu_0}{\sigma / \sqrt{n}} > -1.645 \right]                                                \\
   & = P_{\mu_A} \left[\frac{\overline{X}_n - \mu_A}{\sigma / \sqrt{n}} > \frac{\mu_0 - \mu_A}{\sigma / \sqrt{n}} -1.645 \right]        \\
   & = 1 - P_{\mu_A} \left[\frac{\overline{X}_n - \mu_A}{\sigma / \sqrt{n}} \leq \frac{\mu_0 - \mu_A}{\sigma / \sqrt{n}} -1.645 \right] \\
   & = 1 - \Phi \left( \frac{\mu_0 - \mu_A}{\sigma - \sqrt{n}} - 1.645 \right) \\
   & \text{Weil } \sim \Standardnormalverteilt
\end{align*}
\BoxEnd{}
\begin{definition}{$t$-Test}
\color{red} Normalverteilung, Test für Erwartungswert bei unbekannter Varianz. \color{black} Hier sind
$\zufallsvariablen$ i.i.d. $\sim \Normalverteilt$ unter $P_{\vec{\vt}}$, wobei
$\vec{\vt} = (\mu, \sigma^2)$. Wir wollen die Hypothese $\mu = \mu_0$ testen.
Die Teststatistik ist:
\begin{align*}
  T   & := \frac{\overline{X}_n - \mu_0}{S / \sqrt{n}} \sim t_{n-1}
      &                                                             & \text{unter } P_{\vt_0} \\
  S^2 & = \frac{1}{n-1} \Sn  (X_i - \overline{X}_n)^2
\end{align*}
Und die Verwerfungsbereiche:
\begin{align*}
  c_<      & = t_{n-1, \alpha}               &  & (-\infty, c_<)                     &  & \mu < \mu_0    \\
  c_>      & = t_{n-1, 1 - \alpha}           &  & (c_>, \infty)                      &  & \mu > \mu_0    \\
  c_{\neq} & = t_{n-1, 1 - \frac{\alpha}{2}} &  & (-\infty, c_<) \cup  (c_>, \infty) &  & \mu \neq \mu_0
\end{align*}
Wobei gilt $t_{m, \alpha} = -t_{m, 1 - \alpha}$.
\end{definition}
\subsection{Gepaarter Zweiproben-Test}
Hier sind $\zufallsvariablen$ i.i.d. $\sim \mathcal{N} (\mu_X, \sigma^2)$ und
$Y_1, \dots, Y_n$ i.i.d. $\sim \mathcal{N} (\mu_Y, \sigma^2)$ unter $P_\vt$.
Insbesondere ist $m = n$ und die Varianz beider Stichproben dieselbe.
Differenzen $Z_i := X_I - Y_i$ sind unter $P_\vt$ i.i.d. $\mathcal{N} (\mu_X -
  \mu_Y, 2 \sigma^2)$. Dann analog $z$ und $t$-Test. (Setzt natürliche Paarung
von Daten voraus!)
\subsection{Ungepaarter Zweiproben-Test}
Hier sind unter $P_\vt$ die Zufallsvariablen $\zufallsvariablen$ i.i.d. $\sim
  \mathcal{N} (\mu_X, \sigma^2)$ und $Y_1, \dots, Y_m$ i.i.d. $\sim \mathcal{N}
  (\mu_Y, \sigma^2)$, wobei die Varianz in beiden Fällen dieselbe ist.
\begin{itemize}
  \item Bei bekannter Varianz:
        \begin{align*}
          H_0 & : \mu_X - \mu_Y = \mu_0 \quad  (z.B. \; \mu_0 = 0)                                                                                \\
          T   & = \frac{\overline{X}_n - \overline{Y}_m -  (\mu_X - \mu_Y)}{\sigma \sqrt{\frac{1}{n} + \frac{1}{m}}} \sim \Standardnormalverteilt
        \end{align*}
        Die kritischen Werte für den Verwerfungsbereich sind wie oben
        geeignete Quantile der $\Standardnormalverteilt$-Verteilung, je nach
        Alternative. Das ist der ungepaarte Zweistichproben-$z$-Test.

  \item Bei unbekannter Varianz:
        \begin{align*}
          S_X^2 & := \frac{1}{n-1} \Sn  (X_i - \overline{X}_n)^2                                                                 \\
          S_Y^2 & := \frac{1}{m-1} \sum_{j = 1}^m  (Y_j - \overline{Y}_m)^2                                                      \\
          S^2   & := \frac{1}{m+n-2} \left(  (n-1) \cdot S_X^2 +  (m-1) \cdot S_Y^2 \right)                                      \\
          T     & = \frac{\overline{X}_n - \overline{Y}_m -  (\mu_X - \mu_Y)}{S \sqrt{\frac{1}{n} + \frac{1}{m}}} \sim t_{n+m-2}
        \end{align*}
        unter jedem $P_\vt$. Dieser Test heisst ungepaarter Zweistichproben-$t$-Test.
\end{itemize}
\subsection{Konfidenzbereich}
Ein Konfidenzbereich für $\vt$ zu Daten $x_1, \dots, x_n$ ist eine Menge $C
  (x_1, \dots, x_n) \subseteq \varTheta$. Damit ist $C (\zufallsvariablen)$ eine
zufällige Teilmenge von $\varTheta$. Dieses $C$ heisst Konfidenzbereich zum
Niveau $1 - \alpha$, falls für alle $\vt \in \varTheta$ gilt:
\begin{align*}
  P_\vt [\vt \in C (\zufallsvariablen)] &\geq 1 - \alpha\\
  KI_{Z-wert} &= \bar{x} \pm z \cdot \frac{s}{\sqrt{n}}\\
  KI_{T-wert} &= \bar{x} \pm t \cdot \frac{s}{\sqrt{n}}\\
  \bar{x} &= \text{Mittelwert}\\
  s &= \text{Standardabweichung}\\
  n &= \text{Stichprobengrösse}\\
  z &= \text{Z-Wert}\\
\end{align*}

\BoxStart{}
\subsection{Beispiel: Konfidenzbereich}
Machen wir den Ansatz:
\begin{align*}
  C (\zufallsvariablen) = [\overline{X}_n - \dots, \overline{X}_n + \dots]
\end{align*}
so wollen wir erreichen, dass gilt:
\begin{align*}
  1 - \alpha \leq P_\vt[\vt \in  C (\zufallsvariablen)] \\
  = P_\vt \left[ \mu \in [\overline{X}_n - \dots, \overline{X}_n + \dots] \right]
  = P_\vt \left[ \abs{\overline{X}_n - \mu} \leq \dots \right]
\end{align*}
Nach Satz 7.1 ist für jedes $\vt \in \varTheta$:
\begin{align*}
  \frac{\overline{X}_n - \mu}{S / \sqrt{n}} \sim t_{n-1} &  & \text{unter } P_\vt \\
  1 - \alpha \leq P_\vt \left[ \abs{\frac{\overline{X}_n - \mu}{S / \sqrt{n}}} \leq \frac{\dots}{S / \sqrt{n}} \right]
\end{align*}
also erhalten wir das Konfidenzintervall für $\mu$ zum Niveau $1 - \alpha$:
\begin{align*}
  C (\zufallsvariablen) = \left[ \overline{X}_n - t_{n-1, 1 - \frac{\alpha}{2}} \frac{S}{\sqrt{n}}, \overline{X}_n + t_{n-1, 1 - \frac{\alpha}{2}} \frac{S}{\sqrt{n}} \right]
\end{align*}
\BoxEnd{}
